{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qScmQ6cuANJ8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1731362299754,
     "user": {
      "displayName": "varun Rengarajan",
      "userId": "15175499018942848666"
     },
     "user_tz": -330
    },
    "id": "Wyvit1KIB3ac",
    "outputId": "fc300edc-2354-4df8-9d44-2afc7f078c1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "def eps_greedy(Q, s ,eps=0.1):\n",
    "    if np.random.uniform(0,1) < eps:\n",
    "        return np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        return greedy(Q, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iozYDM8PDws7"
   },
   "source": [
    "**Greedy Policy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "8ycRVu68DjGy",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def greedy(Q, s):\n",
    "    return np.argmax(Q[s])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n5wL8YGxEQUC"
   },
   "source": [
    "**Policy Tesing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH06eV0rECLj"
   },
   "outputs": [],
   "source": [
    "def run_episodes(env, Q, num_episodes=100, to_print=False):\n",
    "    tot_rew = []\n",
    "    state = env.reset()\n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        game_rew = 0\n",
    "\n",
    "        while not done:\n",
    "            next_state, rew, done, _ =env.step(greedy(Q, state))\n",
    "            state = next_state\n",
    "            game_rew += rew\n",
    "\n",
    "            if done:\n",
    "                state=env.reset()\n",
    "                tot_rew.append(game_rew)\n",
    "        if to_print:\n",
    "          print('Mean score: %.3f of %i of games!'%(np.mean(tot_rew), num_episodes))\n",
    "\n",
    "    return np.mean(tot_rew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ND6j8DWzG6Oz"
   },
   "source": [
    "# **Q_Learning**\n",
    "* initialize Q matrix\n",
    "* decay epsilon until reaches the threshold\n",
    "* select action following epsilon Greedy policy\n",
    "* Q-Learning update state&action values\n",
    "* Testing the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Zuz1O4rG3ob"
   },
   "outputs": [],
   "source": [
    "def Q_learning(env, lr=0.01, num_episodes=10000, eps=0.3, gamma=0.95, eps_decay=0.00005):\n",
    "    nA = env.action_space.n\n",
    "    nS = env.observation_space.n\n",
    "\n",
    "    Q = np.zeros((nS, nA))\n",
    "    games_reward = []\n",
    "    test_reward = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        tot_rew = 0\n",
    "\n",
    "        if eps > 0.01:\n",
    "          eps -= eps_decay\n",
    "\n",
    "        while not done:\n",
    "            action = eps_greedy(Q, state, eps)\n",
    "\n",
    "            next_state, rew, done, _ = env.step(action)\n",
    "\n",
    "            Q[state, action] = Q[state, action] + lr*(rew+gamma*np.max(Q[next_state]) - Q[state][action])\n",
    "\n",
    "            state = next_state\n",
    "            tot_rew += rew\n",
    "            if done:\n",
    "                games_reward.append(tot_rew)\n",
    "\n",
    "        if (ep%300) == 0:\n",
    "          test_rew=run_episodes(env, Q, 1000)\n",
    "          print(\"Episodes:{:5d} Eps{:2.4f} Rew{:2.4f}\".format(ep, eps, test_rew))\n",
    "          test_reward.append(test_rew)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmrkdckHMzGa"
   },
   "source": [
    "# **Q-Learning-Taxi v3 Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52164,
     "status": "ok",
     "timestamp": 1731362351911,
     "user": {
      "displayName": "varun Rengarajan",
      "userId": "15175499018942848666"
     },
     "user_tz": -330
    },
    "id": "-hvF6ZfcMrq0",
    "outputId": "69067872-18d5-48e3-e7c3-3da573541f41"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Learning\n",
      "Episodes:    0 Eps0.3990 Rew-200.0000\n",
      "Episodes:  300 Eps0.0990 Rew-199.7850\n",
      "Episodes:  600 Eps0.0100 Rew-208.0780\n",
      "Episodes:  900 Eps0.0100 Rew-147.8630\n",
      "Episodes: 1200 Eps0.0100 Rew-104.7600\n",
      "Episodes: 1500 Eps0.0100 Rew-72.8990\n",
      "Episodes: 1800 Eps0.0100 Rew-61.6740\n",
      "Episodes: 2100 Eps0.0100 Rew-36.5070\n",
      "Episodes: 2400 Eps0.0100 Rew-31.2380\n",
      "Episodes: 2700 Eps0.0100 Rew0.4510\n",
      "Episodes: 3000 Eps0.0100 Rew4.8040\n",
      "Episodes: 3300 Eps0.0100 Rew5.4930\n",
      "Episodes: 3600 Eps0.0100 Rew7.9510\n",
      "Episodes: 3900 Eps0.0100 Rew7.9230\n",
      "Episodes: 4200 Eps0.0100 Rew7.8250\n",
      "Episodes: 4500 Eps0.0100 Rew7.9060\n",
      "Episodes: 4800 Eps0.0100 Rew7.9370\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env = gym.make('Taxi-v3')\n",
    "    print(\"Q-Learning\")\n",
    "    Q_learning=Q_learning(env, lr=.1, num_episodes=5000, eps=0.4, gamma=0.95, eps_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01odLmcBR9qF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
